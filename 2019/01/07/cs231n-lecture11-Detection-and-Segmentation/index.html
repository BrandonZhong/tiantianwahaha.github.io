<!DOCTYPE html><html lang="en"><head><meta name="generator" content="Hexo 3.8.0"><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>cs231n lecture11 Detection and Segmentation | Voila</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/normalize/8.0.0/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.bootcss.com/jquery/3.3.1/jquery.min.js"></script><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">cs231n lecture11 Detection and Segmentation</h1><a id="logo" href="/.">Voila</a><p class="description"></p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> Home</i></a><a href="/archives/"><i class="fa fa-archive"> Archive</i></a><a href="/about/"><i class="fa fa-user"> About</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">cs231n lecture11 Detection and Segmentation</h1><div class="post-meta">Jan 7, 2019<span> | </span><span class="category"><a href="/categories/cs231n/">cs231n</a></span><script src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> Hits</span></span></div><div class="post-content"><p>[TOC]</p>
<p><img src="/2019/01/07/cs231n-lecture11-Detection-and-Segmentation/example.png" alt=""></p>
<h2 id="1-语义分割"><a href="#1-语义分割" class="headerlink" title="1.语义分割"></a>1.语义分割</h2><p>给图片的每个像素都分类，与物体分割不同，只关心像素，而且如果有两个牛在图片里，也不区分划分在一起。</p>
<p>(1) 在图片中滑动选择无数个小区域，每个小区域进行分类，表示中心像素点所属的分类，问题是效率太低。</p>
<p>(2) 直接把图片输入进全卷积网络，输出一个scores : C×H×W的结果，C表示一共几类，然后argmax得出预测结果H×W。在原始图片的大小卷积花费太多内存和时间。</p>
<p>(3) 所以在卷积中间，加入下采样和上采样。</p>
<p>下采样有Pooling, strided convolution</p>
<p>上采样有Unpooling or strided, transpose convolution</p>
<p><strong>上采样有”Unpooling” : Unpooling or strided transpose convolution</strong></p>
<p>Input 2×2 out put 4×4</p>
<p>(1)Nearest Neighbor是指把一个像素复制4份，进行上采样</p>
<p>(2)Bed of Nails是指除了左上角像素直接复制，其他位置的三个像素补0</p>
<p>(3)“Max Unpooling”</p>
<p>在整个卷积网络的工程中，前面的下采样和后面的上采样是一一对应的。记住前面做max pooling的对应元素位置，在做上采样的时候，把元素放在对应的位置，其他的元素位置补0</p>
<h3 id="4-重点：Transpose-Convolution反卷积"><a href="#4-重点：Transpose-Convolution反卷积" class="headerlink" title="(4)重点：Transpose Convolution反卷积"></a><strong>(4)重点：Transpose Convolution反卷积</strong></h3><p>前面提到的都是固定规则的上采样，不需要训练参数，还有可学习的上采样:Transpose Convolution, 在其他地方也称作：Deconvolution (bad), Upconvolution, Fractionally strided convolution, Backward strided convolution。都是一个意思。但是推荐叫Transpose Convolution。</p>
<p><img src="/2019/01/07/cs231n-lecture11-Detection-and-Segmentation/transpose convolution.png" alt=""></p>
<p>3×3的卷积核，步长为2，pad为1. input 2×2，看做是卷积核的权重，像素乘以卷积核，然后值作为上采样的像素。如果和之前的结果有重叠部分，那么直接相加。</p>
<p>有人提出，用3×3步长为2的Transpose Convolution存在checkerboard artifact棋盘效应的问题。所以最新的文章，有人使用4×4步长为2，或者2×2</p>
<h2 id="2-分类加定位"><a href="#2-分类加定位" class="headerlink" title="2.分类加定位"></a>2.分类加定位</h2><p>分类加定位就是分类+边框bounding boxes</p>
<p>一个图片，输入卷积网络中，4096个神经元经过FC，输出Class Scores，有几类就输出几个得分。同时还要输出Box coordinate框坐标(x,y,w,h)，分别是想x和y坐标，框的宽和高，连续值，这是个<strong>回归问题</strong>。</p>
<p><img src="/2019/01/07/cs231n-lecture11-Detection-and-Segmentation/localization.png" alt=""></p>
<p>存在一个问题，如果图片里有很多个物体，要求很多个Box coordinate</p>
<h2 id="3-目标检测"><a href="#3-目标检测" class="headerlink" title="3.目标检测"></a>3.目标检测</h2><p>多物体的话就是目标检测。要用到滑动窗口！</p>
<p>将CNN网络应用在一个图片的各种不同分割，然后CNN给每个分割分类是目标或者背景。存在问题，计算花费太高。</p>
<p>使用Region proposals候选区域/ Selective Search</p>
<p>遍历所有区域花费太高，先用算法找到2000个候选区域，然后对候选区域分类。</p>
<h3 id="R-CNN"><a href="#R-CNN" class="headerlink" title="R-CNN"></a>R-CNN</h3><p>先找到2K个候选区域</p>
<p>找到的区域大小不一，先使用某种方法warp图片，切分为一样大小</p>
<p>然后把每个处理过的区域经过convNet</p>
<p>输出Linear regression for bounding box offesets + classify regions with SVMs</p>
<p>存在问题，训练太慢</p>
<h3 id="Fast-R-CNN"><a href="#Fast-R-CNN" class="headerlink" title="Fast R-CNN"></a>Fast R-CNN</h3><p>整个图片输入convNet网络</p>
<p>在学习到的特征映射中找到候选区域</p>
<p>然后经过”Rol Pooling” layer 将不同的大小处理</p>
<p>剩下的同R-CNN</p>
<h3 id="Faster-R-CNN"><a href="#Faster-R-CNN" class="headerlink" title="Faster R-CNN"></a>Faster R-CNN</h3><p>插入RPN网络，从特征中去预测候选区域</p>
<p>Insert Region Proposal Network (RPN) to predict proposals from features</p>
<p><img src="/2019/01/07/cs231n-lecture11-Detection-and-Segmentation/faster r-cnn.png" alt=""></p>
<p><img src="/2019/01/07/cs231n-lecture11-Detection-and-Segmentation/speed.png" alt=""></p>
<h3 id="YOLO-SSD-Detection-without-Proposals"><a href="#YOLO-SSD-Detection-without-Proposals" class="headerlink" title="YOLO/SSD Detection without Proposals"></a>YOLO/SSD Detection without Proposals</h3><p>把图片划分成grid cell网格单元。设置一系列的base boxes以各个gird cell为中心，假设为B=3，一个宽的，一个长的，一个正方形的。</p>
<p>对每个base boxes去找到最终的一个base boxes，求5个值(x,y,h,w,confidence)</p>
<p>虽然准确率没有之前的模型高，但是最快</p>
<p><img src="/2019/01/07/cs231n-lecture11-Detection-and-Segmentation/SSD.png" alt=""></p>
<h2 id="4-物体分割"><a href="#4-物体分割" class="headerlink" title="4.物体分割"></a>4.物体分割</h2><p>Mask R-CNN(没仔细讲，没明白)</p>
<p>Predict a mask for each of C classes C ×14 ×14</p>
</div><div class="tags"><a href="/tags/cs231n/">cs231n</a></div><div class="post-nav"><a class="next" href="/2019/01/03/Super-FAN-Integrated-facial-landmark-localization-and-super-resolution-of-real-world-low-resolution-faces-in-arbitrary-poses-with-GANS/">Super-FAN:Integrated facial landmark localization and super-resolution of real-world low resolution faces in arbitrary poses with GANS</a></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> Categories</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs231n/">cs231n</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/百篇论文阅读计划/">百篇论文阅读计划</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> Tags</i></div><div class="tagcloud"><a href="/tags/GAN/" style="font-size: 15px;">GAN</a> <a href="/tags/cs231n/" style="font-size: 15px;">cs231n</a> <a href="/tags/图像风格转换/" style="font-size: 15px;">图像风格转换</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> Recent</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2019/01/07/cs231n-lecture11-Detection-and-Segmentation/">cs231n lecture11 Detection and Segmentation</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/01/03/Super-FAN-Integrated-facial-landmark-localization-and-super-resolution-of-real-world-low-resolution-faces-in-arbitrary-poses-with-GANS/">Super-FAN:Integrated facial landmark localization and super-resolution of real-world low resolution faces in arbitrary poses with GANS</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/12/24/cs231n-Lecture-10-Recurrent-Neural-Networks/">cs231n Lecture 10 Recurrent Neural Networks</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/12/22/cs231n-Lecture-9-CNN-Architectures/">cs231n Lecture 9 CNN Architectures</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/12/22/cs231n-Lecture-7-Training-Neural-Networks-part-2/">cs231n Lecture 7 Training Neural Networks part 2</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/12/21/cs231n-Lecture-6-Training-Neural-Networks-part-I/">cs231n Lecture 6 Training Neural Networks, part I</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/12/20/cs231n-参数设置技巧/">cs231n 参数设置技巧</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/12/20/cs231n-Lecture5-Convolutional-Neural-Networks/">cs231n Lecture5 Convolutional Neural Networks</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/11/14/BeautyGAN-Instance-level-Facial-Makeup-Transfer-with-Deep/">BeautyGAN: Instance-level Facial Makeup Transfer with Deep</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/11/10/cs231n-Lecture3-Loss-Functions-and-Optimization/">cs231n Lecture3 Loss Functions and Optimization</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> Links</i></div><ul></ul><a href="https://github.com/tiantianwahaha" title="github" target="_blank">github</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2019 <a href="/." rel="nofollow">Voila.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//cdn.bootcss.com/fancybox/3.3.5/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/fancybox/3.3.5/jquery.fancybox.min.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>