<!DOCTYPE html><html lang="en"><head><meta name="generator" content="Hexo 3.8.0"><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="tiantianwahaha's blogs"><title>tiantianwahaha</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/normalize/8.0.0/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.bootcss.com/jquery/3.3.1/jquery.min.js"></script><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
ga('create','UA-132074008-1','auto');ga('send','pageview');
</script></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">tiantianwahaha</h1><a id="logo" href="/.">tiantianwahaha</a><p class="description"></p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> Home</i></a><a href="/archives/"><i class="fa fa-archive"> Archive</i></a><a href="/about/"><i class="fa fa-user"> About</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title"><a href="/2019/01/12/如何计算卷积层的参数个数.html/">如何计算卷积层的参数个数</a></h1><div class="post-meta">2019-01-12</div><div class="post-content"><p><strong>如何计算卷积层的参数个数</strong></p>
<p>不考虑batchsize，输入64×64×16，经过conv3×3，输出64×64×32，也就是H和W是64，通道数或者说feature map数从16到32.</p>
<p><strong>卷积核个数</strong>。为输出的feature map数，也就是32</p></div><p class="readmore"><a href="/2019/01/12/如何计算卷积层的参数个数.html/">Read More</a></p></div><div class="post"><h1 class="post-title"><a href="/2019/01/12/Fully-Convolutional-Network-FCN.html/">Fully Convolutional Network (FCN)</a></h1><div class="post-meta">2019-01-12</div><div class="post-content"><p><strong>使用卷积层替代CNN末尾的全连接层</strong></p>
<p>全卷积神经网络Fully Convolutional Network (FCN)</p>
<p><a href="https://blog.csdn.net/u010548772/article/details/78582250" target="_blank" rel="noopener">别人的博客介绍</a></p></div><p class="readmore"><a href="/2019/01/12/Fully-Convolutional-Network-FCN.html/">Read More</a></p></div><div class="post"><h1 class="post-title"><a href="/2019/01/12/inception-network.html/">inception network</a></h1><div class="post-meta">2019-01-12</div><div class="post-content"><p><strong>Inception V1-V4</strong></p>
<ol>
<li>介绍4个版本的inception模型</li>
</ol>
<p><a href="https://cloud.tencent.com/developer/article/1146911" target="_blank" rel="noopener">从Inception v1到Inception-ResNet，一文概览Inception家族的「奋斗史」</a></p></div><p class="readmore"><a href="/2019/01/12/inception-network.html/">Read More</a></p></div><div class="post"><h1 class="post-title"><a href="/2019/01/09/Show-Attend-and-Tell-Neural-Image-Caption-Generation-with-Visual-Attention.html/">Show, Attend and Tell: Neural Image Caption Generation with Visual Attention</a></h1><div class="post-meta">2019-01-09</div><div class="post-content"><p>本文是2015年的一篇根据图片生成文字描述的文章。</p>
<p>主要是因为文章里用了soft attention和hard attention</p>
<p><strong>详细内容可以参考<a href="http://www.cosmosshadow.com/ml/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/2016/03/08/Attention.html" target="_blank" rel="noopener">博客</a>!!(需要翻墙！)</strong></p></div><p class="readmore"><a href="/2019/01/09/Show-Attend-and-Tell-Neural-Image-Caption-Generation-with-Visual-Attention.html/">Read More</a></p></div><div class="post"><h1 class="post-title"><a href="/2019/01/09/Spectral-Normalization-for-Generative-Adversarial-Networks.html/">Spectral Normalization for Generative Adversarial Networks</a></h1><div class="post-meta">2019-01-09</div><div class="post-content"><p>谱归一化</p>
<p>GAN训练不稳定，通过在对抗器加入谱归一化，在每层对权重进行归一化操作，稳定训练，同时还起到了权重正则化的作用。最终得到了更好的图像效果，一个模型在多类别图像上都表现的很好。</p>
<p>进过推论证明，在每层对权重归一化，就相当于让权重除以他的对应矩阵范数，求矩阵范数需要计算这个矩阵的特征值，可以通过幂迭代法来近似求取。</p></div><p class="readmore"><a href="/2019/01/09/Spectral-Normalization-for-Generative-Adversarial-Networks.html/">Read More</a></p></div><div class="post"><h1 class="post-title"><a href="/2019/01/08/Self-Attention-Generative-Adversarial-Networks.html/">Self-Attention Generative Adversarial Networks</a></h1><div class="post-meta">2019-01-08</div><div class="post-content"><p>自注意力生成对抗网络，首次把自注意力引入生成对抗网络图像生成中，结合使用谱归一化，得到了更好的图像质量。</p>
<p>Self-Attention Generative Adversarial Network(SAGAN)是一个注意力驱动，长范围，关联模型(attention-driven, long-range dependency modeling )。</p></div><p class="readmore"><a href="/2019/01/08/Self-Attention-Generative-Adversarial-Networks.html/">Read More</a></p></div><div class="post"><h1 class="post-title"><a href="/2019/01/07/cs231n-lecture11-Detection-and-Segmentation.html/">cs231n lecture11 Detection and Segmentation</a></h1><div class="post-meta">2019-01-07</div><div class="post-content"><ul>
<li>语义分割 (重点有：Transpose convolution反卷积)</li>
<li>分类+定位</li>
<li>目标检测 (RNN, Faster RNN, YOLO)</li>
<li>物体分割</li></ul></div><p class="readmore"><a href="/2019/01/07/cs231n-lecture11-Detection-and-Segmentation.html/">Read More</a></p></div><div class="post"><h1 class="post-title"><a href="/2019/01/03/Super-FAN-Integrated-facial-landmark-localization-and-super-resolution-of-real-world-low-resolution-faces-in-arbitrary-poses-with-GANS.html/">Super-FAN:Integrated facial landmark localization and super-resolution of real-world low resolution faces in arbitrary poses with GANS</a></h1><div class="post-meta">2019-01-03</div><div class="post-content"><p>使用GAN将现实世界中任意姿态下低分辨率人脸的人脸关键点定位和超分辨率结合起来。</p>
<p>CVPR2018 spotlignt</p>
<p><a href="https://www.adrianbulat.com/" target="_blank" rel="noopener">作者主页</a></p></div><p class="readmore"><a href="/2019/01/03/Super-FAN-Integrated-facial-landmark-localization-and-super-resolution-of-real-world-low-resolution-faces-in-arbitrary-poses-with-GANS.html/">Read More</a></p></div><div class="post"><h1 class="post-title"><a href="/2018/12/24/cs231n-Lecture-10-Recurrent-Neural-Networks.html/">cs231n Lecture 10 Recurrent Neural Networks</a></h1><div class="post-meta">2018-12-24</div><div class="post-content"><ul>
<li>RNN</li>
<li>Language modeling(RNN) </li>
<li>Image captioning, Soft attention, visual question answering(CNN+RNN)</li>
<li>LSTM, GRU </li></ul></div><p class="readmore"><a href="/2018/12/24/cs231n-Lecture-10-Recurrent-Neural-Networks.html/">Read More</a></p></div><div class="post"><h1 class="post-title"><a href="/2018/12/22/cs231n-Lecture-9-CNN-Architectures.html/">cs231n Lecture 9 CNN Architectures</a></h1><div class="post-meta">2018-12-22</div><div class="post-content"><p>介绍几个重要的CNN模型(在imagenet比赛中获得冠军)</p>
<ul>
<li>AlexNet</li>
<li>VGG</li>
<li>GoogleNet</li></ul></div><p class="readmore"><a href="/2018/12/22/cs231n-Lecture-9-CNN-Architectures.html/">Read More</a></p></div><nav class="page-navigator"><span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/">Next</a></nav><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML" async></script></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> Categories</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs231n/">cs231n</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/deep-learning/">deep learning</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/百篇论文阅读计划/">百篇论文阅读计划</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> Tags</i></div><div class="tagcloud"><a href="/tags/GAN/" style="font-size: 15px;">GAN</a> <a href="/tags/deep-learning/" style="font-size: 15px;">deep learning</a> <a href="/tags/image-caption/" style="font-size: 15px;">image caption</a> <a href="/tags/cs231n/" style="font-size: 15px;">cs231n</a> <a href="/tags/图像风格转换/" style="font-size: 15px;">图像风格转换</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> Recent</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2019/01/12/如何计算卷积层的参数个数.html/">如何计算卷积层的参数个数</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/01/12/Fully-Convolutional-Network-FCN.html/">Fully Convolutional Network (FCN)</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/01/12/inception-network.html/">inception network</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/01/09/Show-Attend-and-Tell-Neural-Image-Caption-Generation-with-Visual-Attention.html/">Show, Attend and Tell: Neural Image Caption Generation with Visual Attention</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/01/09/Spectral-Normalization-for-Generative-Adversarial-Networks.html/">Spectral Normalization for Generative Adversarial Networks</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/01/08/Self-Attention-Generative-Adversarial-Networks.html/">Self-Attention Generative Adversarial Networks</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/01/07/cs231n-lecture11-Detection-and-Segmentation.html/">cs231n lecture11 Detection and Segmentation</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/01/03/Super-FAN-Integrated-facial-landmark-localization-and-super-resolution-of-real-world-low-resolution-faces-in-arbitrary-poses-with-GANS.html/">Super-FAN:Integrated facial landmark localization and super-resolution of real-world low resolution faces in arbitrary poses with GANS</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/12/24/cs231n-Lecture-10-Recurrent-Neural-Networks.html/">cs231n Lecture 10 Recurrent Neural Networks</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/12/22/cs231n-Lecture-9-CNN-Architectures.html/">cs231n Lecture 9 CNN Architectures</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> Links</i></div><ul></ul><a href="https://github.com/tiantianwahaha" title="github" target="_blank">github</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2019 <a href="/." rel="nofollow">tiantianwahaha.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//cdn.bootcss.com/fancybox/3.3.5/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/fancybox/3.3.5/jquery.fancybox.min.css"><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>