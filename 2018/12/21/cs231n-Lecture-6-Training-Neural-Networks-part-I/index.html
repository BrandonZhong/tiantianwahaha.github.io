<!DOCTYPE html><html lang="en"><head><meta name="generator" content="Hexo 3.8.0"><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>cs231n Lecture 6 Training Neural Networks, part I | Voila</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/normalize/8.0.0/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.bootcss.com/jquery/3.3.1/jquery.min.js"></script><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">cs231n Lecture 6 Training Neural Networks, part I</h1><a id="logo" href="/.">Voila</a><p class="description"></p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> Home</i></a><a href="/archives/"><i class="fa fa-archive"> Archive</i></a><a href="/about/"><i class="fa fa-user"> About</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">cs231n Lecture 6 Training Neural Networks, part I</h1><div class="post-meta">Dec 21, 2018<span> | </span><span class="category"><a href="/categories/cs231n/">cs231n</a></span><script src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> Hits</span></span></div><div class="post-content"><ul>
<li><p>激活函数</p>
</li>
<li><p>数据预处理</p>
</li>
<li>权重初始化</li>
<li><p>批量归一化</p>
</li>
<li><p>超参数优化</p>
</li>
</ul>
<p><a href="http://cs231n.stanford.edu/syllabus.html" target="_blank" rel="noopener">课程官方链接</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/21930884" target="_blank" rel="noopener">课程笔记翻译</a></p>
<h2 id="1-激活函数"><a href="#1-激活函数" class="headerlink" title="1.激活函数"></a>1.激活函数</h2><p><img src="/2018/12/21/cs231n-Lecture-6-Training-Neural-Networks-part-I/1.png" alt=""></p>
<h3 id="1-Sigmoid"><a href="#1-Sigmoid" class="headerlink" title="(1)Sigmoid"></a>(1)Sigmoid</h3><p><img src="/2018/12/21/cs231n-Lecture-6-Training-Neural-Networks-part-I/2.png" alt=""></p>
<p>sigmoid讲输入压缩到0到1的范围内，曾经非常常用，现在已经很少使用了。主要有两个缺点，（1）Sigmoid函数饱和使梯度消失。当神经元的激活在接近0或1处时会饱和：在这些区域，梯度几乎为0。（2）Sigmoid函数的输出不是零中心的。因为如果输入神经元的数据总是正数（比如在$f=w^Tx+b$中每个元素都x&gt;0），那么关于w的梯度在反向传播的过程中，将会要么全部是正数，要么全部是负数（具体依整个表达式f而定）。这将会导致梯度下降权重更新时出现z字型的下降。然而，可以看到整个批量的数据的梯度被加起来后，对于权重的最终更新将会有不同的正负，这样就从一定程度上减轻了这个问题。因此，该问题相对于上面的神经元饱和问题来说只是个小麻烦，没有那么严重。</p>
<h3 id="（2）tanh"><a href="#（2）tanh" class="headerlink" title="（2）tanh"></a>（2）tanh</h3><p><img src="/2018/12/21/cs231n-Lecture-6-Training-Neural-Networks-part-I/3.png" alt=""></p>
<p>将实数值压缩到[-1,1]之间,它也存在饱和问题，但是和sigmoid神经元不同的是，它的输出是零中心的.注意tanh神经元是一个简单放大的sigmoid神经元</p>
<h3 id="（3）ReLU"><a href="#（3）ReLU" class="headerlink" title="（3）ReLU"></a>（3）ReLU</h3><p><img src="/2018/12/21/cs231n-Lecture-6-Training-Neural-Networks-part-I/4.png" alt=""></p>
<p>x&gt;0,斜率为1，x&lt;=0时，斜率为0.论文证明使用ReLU比使用tanh的收敛快6倍。sigmoid和tanh神经元含有指数运算等耗费计算资源的操作，而ReLU可以简单地通过对一个矩阵进行阈值计算得到。</p>
<p>缺点是，ReLU的单元容易死掉，如果学习率设置的太高，网络汇总40%的神经元都会死掉</p>
<h3 id="（4）Leaky-ReLU"><a href="#（4）Leaky-ReLU" class="headerlink" title="（4）Leaky ReLU"></a>（4）Leaky ReLU</h3><p><img src="/2018/12/21/cs231n-Lecture-6-Training-Neural-Networks-part-I/5.png" alt=""></p>
<p>Leaky ReLU解决单元死亡问题，x&lt;0的时候给一个小小的梯度值</p>
<h3 id="（5）其他"><a href="#（5）其他" class="headerlink" title="（5）其他"></a>（5）其他</h3><p>Exponential Linear Units (ELU)</p>
<p>Maxout “Neuron”</p>
<h3 id="（6）总结"><a href="#（6）总结" class="headerlink" title="（6）总结"></a>（6）总结</h3><blockquote>
<p><strong>用ReLU非线性函数，注意设置好学习率</strong></p>
<p><strong>如果单元死亡问题困扰你，就试试Leaky ReLU / Maxout / ELU</strong></p>
<p><strong>也可以试试tanh，但是其效果应该不如ReLU或者Maxout</strong></p>
<p><strong>不要使用sigmoid</strong></p>
</blockquote>
<h2 id="2-数据预处理"><a href="#2-数据预处理" class="headerlink" title="2.数据预处理"></a>2.数据预处理</h2><ul>
<li>中心化(零均值)</li>
<li>归一化，一种是先零中心化，然后除以标准差。一种是全都归一化到[-1,1]之间。</li>
<li>按照上面数据预处理之后，对权重小的扰动不敏感，训练稳定</li>
</ul>
<h2 id="3-权重初始化"><a href="#3-权重初始化" class="headerlink" title="3.权重初始化"></a>3.权重初始化</h2><p>gaussian with zero mean and 1e-2 standard deviation</p>
<p>$W = 0.01 * np.random.randn(D,H)​$</p>
<ul>
<li>权重太小，会导致梯度消失。因为反向传播的时候会乘以当前梯度</li>
<li>权重太多，会导致饱和 。比如tanh。</li>
</ul>
<blockquote>
<p><strong>总结：当前的推荐是使用ReLU激活函数，并且使用$w = np.random.randn(n) * sqrt(2.0/n)$来进行权重初始化，来自<a href="http://link.zhihu.com/?target=http%3A//arxiv-web3.library.cornell.edu/abs/1502.01852" target="_blank" rel="noopener">论文</a>.</strong></p>
</blockquote>
<p><img src="/2018/12/21/cs231n-Lecture-6-Training-Neural-Networks-part-I/9.png" alt=""></p>
<h2 id="4-Batch-Normalization批量归一化"><a href="#4-Batch-Normalization批量归一化" class="headerlink" title="4.Batch Normalization批量归一化"></a>4.Batch Normalization批量归一化</h2><p><img src="/2018/12/21/cs231n-Lecture-6-Training-Neural-Networks-part-I/6.png" alt=""></p>
<p>在全连接层或者卷积层 与 激活函数之间添加一个BatchNorm层。可以理解为在网络的每一层之前都做数据预处理，添加了之后不用小心的进行权重初始化了。</p>
<h2 id="5-Babysitting-the-Learning-Process"><a href="#5-Babysitting-the-Learning-Process" class="headerlink" title="5.Babysitting the Learning Process"></a>5.Babysitting the Learning Process</h2><p>1.预处理数据</p>
<p>2.选择网络结构</p>
<p>3.确定损失函数是合理的：看你的模型是否能够过拟合小数据集</p>
<h2 id="6-超参数优化"><a href="#6-超参数优化" class="headerlink" title="6.超参数优化"></a>6.超参数优化</h2><p>交叉验证，每个跑5epoch，学习率在某个取值范围内测试（best to optimize in log space）</p>
<p>损失函数不变，学习率太小。损失函数NAN，学习率太大。在1e-3到1e-5范围内一般。</p>
<p>10**uniform（-3,-4）</p>
<p>随机搜索&gt;网格搜索</p>
<p><img src="/2018/12/21/cs231n-Lecture-6-Training-Neural-Networks-part-I/7.png" alt=""></p>
<p><img src="/2018/12/21/cs231n-Lecture-6-Training-Neural-Networks-part-I/8.png" alt=""></p>
<p>损失函数在训练集上表现好，在验证集上表现差，说明过拟合了，要加大正则化损失。</p>
<h2 id="7-总结"><a href="#7-总结" class="headerlink" title="7.总结"></a>7.总结</h2><ul>
<li>Activation Functions (use ReLU)</li>
<li>Data Preprocessing (images: subtract mean)</li>
<li>Weight Initialization (use Xavier/He init)</li>
<li>Batch Normalization (use)</li>
<li>Babysitting the Learning process</li>
<li>Hyperparameter Optimization<br>(random sample hyperparams, in log space when appropriate)</li>
</ul>
</div><div class="tags"><a href="/tags/cs231n/">cs231n</a></div><div class="post-nav"><a class="pre" href="/2018/12/22/cs231n-Lecture-7-Training-Neural-Networks-part-2/">cs231n Lecture 7 Training Neural Networks part 2</a><a class="next" href="/2018/12/20/cs231n-参数设置技巧/">cs231n 参数设置技巧</a></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> Categories</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs231n/">cs231n</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/百篇论文阅读计划/">百篇论文阅读计划</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> Tags</i></div><div class="tagcloud"><a href="/tags/cs231n/" style="font-size: 15px;">cs231n</a> <a href="/tags/GAN/" style="font-size: 15px;">GAN</a> <a href="/tags/图像风格转换/" style="font-size: 15px;">图像风格转换</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> Recent</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2018/12/24/cs231n-Lecture-10-Recurrent-Neural-Networks/">cs231n Lecture 10 Recurrent Neural Networks</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/12/22/cs231n-Lecture-9-CNN-Architectures/">cs231n Lecture 9 CNN Architectures</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/12/22/cs231n-Lecture-7-Training-Neural-Networks-part-2/">cs231n Lecture 7 Training Neural Networks part 2</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/12/21/cs231n-Lecture-6-Training-Neural-Networks-part-I/">cs231n Lecture 6 Training Neural Networks, part I</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/12/20/cs231n-参数设置技巧/">cs231n 参数设置技巧</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/12/20/cs231n-Lecture5-Convolutional-Neural-Networks/">cs231n Lecture5 Convolutional Neural Networks</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/11/14/BeautyGAN-Instance-level-Facial-Makeup-Transfer-with-Deep/">BeautyGAN: Instance-level Facial Makeup Transfer with Deep</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/11/10/cs231n-Lecture3-Loss-Functions-and-Optimization/">cs231n Lecture3 Loss Functions and Optimization</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/11/07/cs231n-Lecture2-Image-Classification/">cs231n Lecture2 Image Classification</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/10/31/风格转换-二-：Perceptual-Losses-for-Real-Time-Style-Transfer-and-Super-Resolution/">风格转换(二)：Perceptual Losses for Real-Time Style Transfer and Super-Resolution</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> Links</i></div><ul></ul><a href="https://github.com/tiantianwahaha" title="github" target="_blank">github</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2018 <a href="/." rel="nofollow">Voila.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//cdn.bootcss.com/fancybox/3.3.5/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/fancybox/3.3.5/jquery.fancybox.min.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>